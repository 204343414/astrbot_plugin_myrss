import os
import json
import re
import time
import random
import base64
import logging
import asyncio
import aiohttp
import calendar
from io import BytesIO
from dataclasses import dataclass
from urllib.parse import urlparse
from datetime import datetime
from typing import List

from lxml import etree
from bs4 import BeautifulSoup
from PIL import Image, ImageDraw, ImageFont
from apscheduler.schedulers.asyncio import AsyncIOScheduler

from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult, MessageChain
from astrbot.api.star import Context, Star, register
from astrbot.api import AstrBotConfig
import astrbot.api.message_components as Comp


@dataclass
class RSSItem:
    chan_title: str
    title: str
    link: str
    description: str
    pubDate: str
    pubDate_timestamp: int
    pic_urls: list


class DataHandler:
    def __init__(self, config_path="data/astrbot_plugin_myrss_data.json"):
        self.config_path = config_path
        self.data = self._load()

    def _load(self):
        if not os.path.exists(self.config_path):
            d = {"rsshub_endpoints": []}
            with open(self.config_path, "w", encoding="utf-8") as f:
                json.dump(d, f, indent=2, ensure_ascii=False)
            return d
        with open(self.config_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def save(self):
        with open(self.config_path, "w", encoding="utf-8") as f:
            json.dump(self.data, f, indent=2, ensure_ascii=False)

    def get_subs(self, user_id):
        urls = []
        for url, info in self.data.items():
            if url in ("rsshub_endpoints", "settings"):
                continue
            if user_id in info.get("subscribers", {}):
                urls.append(url)
        return urls

    def parse_channel_info(self, text):
        root = etree.fromstring(text)
        title = root.xpath("//title")[0].text
        desc_nodes = root.xpath("//description")
        desc = desc_nodes[0].text if desc_nodes else ""
        return title, desc or ""

    def strip_html_pic(self, html):
        """ä»HTMLä¸­æå–æ‰€æœ‰å›¾ç‰‡URLï¼ŒåŒ…å«æš´åŠ›æ­£åˆ™åŒ¹é…YouTubeå°é¢"""
        if not html:
            return []
        
        soup = BeautifulSoup(html, "html.parser")
        urls = []
        
        # 1. å¸¸è§„ <img src="...">
        for img in soup.find_all("img"):
            src = img.get("src")
            if src and src not in urls:
                urls.append(src)
                
        # 2. <video poster="...">
        for vid in soup.find_all("video"):
            poster = vid.get("poster")
            if poster and poster not in urls:
                urls.append(poster)
                
        # 3. [æš´åŠ›å¢å¼º] ç›´æ¥æ­£åˆ™æ‰«ææ•´ä¸ªHTMLæ–‡æœ¬åŒ¹é…YouTube ID
        # å› ä¸ºæœ‰æ—¶å€™ RSSHub è¿”å›çš„ description é‡Œåªæœ‰çº¯æ–‡æœ¬é“¾æ¥ï¼Œæ²¡æœ‰ <a> æ ‡ç­¾
        # åŒ¹é… youtube.com/watch?v=xxx æˆ– youtu.be/xxx
        patterns = [
            r'youtube\.com/watch\?v=([\w-]{11})',
            r'youtu\.be/([\w-]{11})',
            r'youtube\.com/embed/([\w-]{11})',
            r'youtube\.com/v/([\w-]{11})'
        ]
        
        found_ids = set()
        # å…ˆæœ soup é‡Œçš„ a æ ‡ç­¾
        for a in soup.find_all("a", href=True):
            for pat in patterns:
                m = re.search(pat, a["href"])
                if m: found_ids.add(m.group(1))

        # å†æš´åŠ›æœå…¨æ–‡ï¼ˆå…œåº•ï¼‰
        for pat in patterns:
            for vid_id in re.findall(pat, html):
                found_ids.add(vid_id)

        # æ„é€ å°é¢åœ°å€
        for vid_id in found_ids:
            # å­˜ä¸¤ä¸ªåˆ†è¾¨ç‡ï¼Œä¼˜å…ˆé«˜æ¸…(maxres)ï¼Œå…¶æ¬¡ä¸­ç­‰(hq)ï¼Œé˜²æ­¢maxresä¸å­˜åœ¨
            u1 = f"https://i.ytimg.com/vi/{vid_id}/maxresdefault.jpg"
            u2 = f"https://i.ytimg.com/vi/{vid_id}/hqdefault.jpg"
            if u1 not in urls: urls.append(u1)
            if u2 not in urls: urls.append(u2)
        
        return urls

    def strip_html(self, html):
        soup = BeautifulSoup(html, "html.parser")
        return re.sub(r"\n+", "\n", soup.get_text())

    def get_root_url(self, url):
        p = urlparse(url)
        return f"{p.scheme}://{p.netloc}"


class PicHandler:
    def __init__(self, adjust=False):
        self.adjust = adjust

    async def to_base64(self, image_url):
        try:
            conn = aiohttp.TCPConnector(ssl=False)
            async with aiohttp.ClientSession(trust_env=True, connector=conn) as s:
                async with s.get(image_url, timeout=aiohttp.ClientTimeout(total=15)) as r:
                    if r.status != 200:
                        return None
                    raw = BytesIO(await r.read())
                    if self.adjust:
                        img = Image.open(raw).convert("RGB")
                        w, h = img.size
                        px = img.load()
                        cx, cy = random.choice([(0, 0), (w - 1, 0), (0, h - 1), (w - 1, h - 1)])
                        px[cx, cy] = (255, 255, 255)
                        buf = BytesIO()
                        img.save(buf, format="JPEG")
                        buf.seek(0)
                        return base64.b64encode(buf.read()).decode()
                    else:
                        return base64.b64encode(raw.getvalue()).decode()
        except Exception:
            return None


class URLMapper:
    RULES = [
        (r"space\.bilibili\.com/(\d+)", "/bilibili/user/video/{0}", "Bç«™UPä¸»è§†é¢‘"),
        (r"bilibili\.com/bangumi/media/md(\d+)", "/bilibili/bangumi/media/{0}", "Bç«™ç•ªå‰§"),
        (r"live\.bilibili\.com/(\d+)", "/bilibili/live/room/{0}", "Bç«™ç›´æ’­é—´"),
        (r"manga\.bilibili\.com/detail/mc(\d+)", "/bilibili/manga/update/{0}", "Bç«™æ¼«ç”»"),
        (r"youtube\.com/channel/([\w-]+)", "/youtube/channel/{0}", "YouTubeé¢‘é“"),
        # [ä¿®å¤] ä¼˜å…ˆåŒ¹é… YouTube çš„åŠ¨æ€(community/posts)ã€Shortsã€ç›´æ’­ç­‰ç‰¹å®šé¡µé¢
        # å¿…é¡»æ”¾åœ¨é€šç”¨çš„ @user è§„åˆ™ä¹‹å‰ï¼Œå¦åˆ™ä¼šè¢«é€šç”¨è§„åˆ™æ‹¦æˆª
        (r"youtube\.com/@([\w.-]+)/(?:posts|community)", "/youtube/community/@{0}", "YouTubeåŠ¨æ€"),
        (r"youtube\.com/@([\w.-]+)/shorts", "/youtube/user/@{0}/shorts", "YouTube Shorts"),
        (r"youtube\.com/@([\w.-]+)/streams", "/youtube/user/@{0}/live", "YouTubeç›´æ’­è®°å½•"),
        # [åŸè§„åˆ™] é€šç”¨ç”¨æˆ·è§„åˆ™æ”¾åœ¨æœ€åä½œä¸ºå…œåº•
        (r"youtube\.com/@([\w.-]+)", "/youtube/user/@{0}", "YouTubeç”¨æˆ·"),
        (r"youtube\.com/playlist\?list=([\w-]+)", "/youtube/playlist/{0}", "YouTubeæ’­æ”¾åˆ—è¡¨"),
        (r"(?:twitter|x)\.com/(?!home|explore|search|settings|i/)([\w]+)", "/twitter/user/{0}", "Twitter/X"),
        (r"weibo\.com/u/(\d+)", "/weibo/user/{0}", "å¾®åš"),
        (r"zhihu\.com/people/([\w-]+)", "/zhihu/people/activities/{0}", "çŸ¥ä¹"),
        (r"zhihu\.com/column/([\w-]+)", "/zhihu/zhuanlan/{0}", "çŸ¥ä¹ä¸“æ "),
        (r"xiaohongshu\.com/user/profile/([\w]+)", "/xiaohongshu/user/{0}/notes", "å°çº¢ä¹¦"),
        (r"github\.com/([\w.-]+)/([\w.-]+)/releases", "/github/release/{0}/{1}", "GitHub Release"),
        (r"github\.com/([\w.-]+)/([\w.-]+)(?:$|[/?#])", "/github/commits/{0}/{1}", "GitHubä»“åº“"),
        (r"github\.com/([\w.-]+)(?:$|[/?#])", "/github/repos/{0}", "GitHubç”¨æˆ·"),
        (r"t\.me/s?/?([\w]+)", "/telegram/channel/{0}", "Telegram"),
        (r"douyin\.com/user/([\w]+)", "/douyin/user/{0}", "æŠ–éŸ³"),
        (r"instagram\.com/([\w.]+)(?:$|[/?#])", "/instagram/user/{0}", "Instagram"),
        (r"pixiv\.net/users/(\d+)", "/pixiv/user/{0}", "Pixiv"),
        (r"sspai\.com/u/([\w]+)", "/sspai/author/{0}", "å°‘æ•°æ´¾"),
        (r"okjike\.com/u/([\w-]+)", "/jike/user/{0}", "å³åˆ»"),
        (r"podcasts\.apple\.com/.*/id(\d+)", "/apple/podcast/{0}", "Apple Podcast"),
    ]

    HINTS = {
        "bilibili": (
            "Bç«™å¯ç”¨è·¯ç”±(uidåœ¨space.bilibili.com/{uid}æ‰¾):\n"
            "  UPä¸»è§†é¢‘: /bilibili/user/video/{uid}\n"
            "  UPä¸»åŠ¨æ€: /bilibili/user/dynamic/{uid}\n"
            "  æ‰€æœ‰è§†é¢‘: /bilibili/user/video-all/{uid}\n"
            "  UPä¸»å›¾æ–‡: /bilibili/user/article/{uid}\n"
            "  UPä¸»åˆé›†: /bilibili/user/collection/{uid}/{sid}\n"
            "  ç»¼åˆçƒ­é—¨: /bilibili/popular/all\n"
            "  æ¯å‘¨å¿…çœ‹: /bilibili/weekly\n"
            "  æ’è¡Œæ¦œ: /bilibili/ranking/all\n"
            "  çƒ­æœ: /bilibili/hot-search\n"
            "  ç•ªå‰§: /bilibili/bangumi/media/{mediaid}\n"
            "  ç›´æ’­: /bilibili/live/room/{roomID}\n"
            "  æœç´¢: /bilibili/vsearch/{keyword}"
        ),
        "youtube": "YouTubeè·¯ç”±:\n  é¢‘é“: /youtube/channel/{id}\n  ç”¨æˆ·: /youtube/user/@{name}\n  æ’­æ”¾åˆ—è¡¨: /youtube/playlist/{id}",
        "twitter": "Twitter/Xè·¯ç”±:\n  ç”¨æˆ·: /twitter/user/{name}\n  åª’ä½“: /twitter/media/{name}\n  æœç´¢: /twitter/keyword/{kw}",
        "x.com": "Twitter/Xè·¯ç”±:\n  ç”¨æˆ·: /twitter/user/{name}\n  åª’ä½“: /twitter/media/{name}",
        "weibo": "å¾®åšè·¯ç”±:\n  ç”¨æˆ·: /weibo/user/{uid}\n  çƒ­æœ: /weibo/search/hot",
        "zhihu": "çŸ¥ä¹è·¯ç”±:\n  ç”¨æˆ·: /zhihu/people/activities/{id}\n  ä¸“æ : /zhihu/zhuanlan/{id}\n  çƒ­æ¦œ: /zhihu/hot",
        "github": "GitHubè·¯ç”±:\n  Release: /github/release/{owner}/{repo}\n  Commits: /github/commits/{owner}/{repo}",
        "xiaohongshu": "å°çº¢ä¹¦è·¯ç”±:\n  ç”¨æˆ·ç¬”è®°: /xiaohongshu/user/{id}/notes",
        "douyin": "æŠ–éŸ³è·¯ç”±:\n  ç”¨æˆ·: /douyin/user/{uid}",
        "instagram": "Instagramè·¯ç”±:\n  ç”¨æˆ·: /instagram/user/{name}",
        "telegram": "Telegramè·¯ç”±:\n  é¢‘é“: /telegram/channel/{name}",
        "pixiv": "Pixivè·¯ç”±:\n  ç”¨æˆ·: /pixiv/user/{uid}\n  æ’è¡Œ: /pixiv/ranking/{mode}",
    }

    @classmethod
    def match(cls, url):
        for pat, tpl, name in cls.RULES:
            m = re.search(pat, url)
            if m:
                return tpl.format(*m.groups()), name
        return None

    @classmethod
    def suggest(cls, url):
        try:
            netloc = urlparse(url).netloc.lower()
        except Exception:
            return "æ— æ³•è§£æï¼Œè¯·æä¾›httpå¼€å¤´çš„é“¾æ¥æˆ–/å¼€å¤´çš„è·¯ç”±ã€‚"
        for kw, hint in cls.HINTS.items():
            if kw in netloc:
                return hint
        return "æœªæ”¶å½•æ­¤å¹³å°ã€‚è¯·åˆ° https://docs.rsshub.app æŸ¥æ‰¾è·¯ç”±åç”¨/å¼€å¤´è°ƒç”¨ã€‚"


class CardGen:
    def __init__(self, width=480):
        self.w = width
        self.pad = 22
        self.font_path = self._find()

    def _find(self):
        base_dir = os.path.dirname(__file__)
        root_fonts = []
        for fn in os.listdir(base_dir):
            lower = fn.lower()
            if lower.endswith((".ttf", ".otf", ".ttc")):
                root_fonts.append(os.path.join(base_dir, fn))
        if root_fonts:
            return root_fonts[0]

        fonts_dir = os.path.join(os.path.dirname(__file__), "fonts")
        if os.path.isdir(fonts_dir):
            files = []
            for fn in os.listdir(fonts_dir):
                lower = fn.lower()
                if lower.endswith((".ttf", ".otf", ".ttc")):
                    files.append(fn)

            def score(name: str) -> int:
                n = name.lower()
                s = 0
                if "notosanscjk" in n or "noto sans cjk" in n:
                    s += 100
                if "notosansjp" in n or "noto sans jp" in n:
                    s += 90
                if "notosanssc" in n or "noto sans sc" in n:
                    s += 80
                if "cjk" in n:
                    s += 70
                if "jp" in n or "japan" in n:
                    s += 60
                if "sc" in n or "chinese" in n:
                    s += 50
                if "minecraft" in n:
                    s += 40
                if "ä¸­æ–‡" in name:
                    s += 30
                return -s

            files.sort(key=score)
            if files:
                return os.path.join(fonts_dir, files[0])

        for p in [
            "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc",
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
            "/System/Library/Fonts/PingFang.ttc",
            "C:\\Windows\\Fonts\\msyh.ttc",
        ]:
            if os.path.exists(p):
                return p
        return None

    def _f(self, sz):
        if self.font_path:
            try:
                return ImageFont.truetype(self.font_path, sz)
            except Exception:
                pass
        return ImageFont.load_default()

    def _wrap(self, txt, font, mw, draw):
        # [ä¿®å¤] æ›´å¥å£®çš„æ¢è¡Œé€»è¾‘ï¼Œé˜²æ­¢æŸäº›ç‰¹æ®Šå­—ç¬¦å¯¼è‡´å´©æºƒ
        if not txt:
            return []
        lines = []
        # å°†æ–‡æœ¬æŒ‰æ®µè½åˆ†å‰²ï¼Œä¿ç•™ç©ºè¡Œ
        paragraphs = txt.split("\n")
        
        for para in paragraphs:
            # ç§»é™¤é¦–å°¾ç©ºç™½ï¼Œä½†å¦‚æœæ˜¯ç©ºè¡Œåˆ™ä¿ç•™é«˜åº¦
            if not para:
                lines.append("")
                continue
            
            # é€å­—æ‰«æ
            current_line = ""
            for char in para:
                # å°è¯•åŠ å…¥å­—ç¬¦
                test_line = current_line + char
                # è·å–å®½åº¦
                w = draw.textlength(test_line, font=font)
                if w > mw:
                    # å¦‚æœè¶…å®½ï¼Œä¸”å½“å‰è¡Œä¸ä¸ºç©ºï¼Œåˆ™æ¨å…¥ä¸Šä¸€è¡Œ
                    if current_line:
                        lines.append(current_line)
                        current_line = char
                    else:
                        # å¼ºåˆ¶åˆ‡æ–­ï¼ˆé’ˆå¯¹è¶…é•¿è¿ç»­å­—ç¬¦ï¼‰
                        lines.append(char)
                        current_line = ""
                else:
                    current_line = test_line
            if current_line:
                lines.append(current_line)
        return lines
    def _round_image(self, img, radius=14):
        """ç»™å›¾ç‰‡åŠ åœ†è§’æ•ˆæœ
        åŸç†ï¼šç”»ä¸€ä¸ªåœ†è§’çŸ©å½¢ç™½è‰²è’™ç‰ˆï¼ŒæŠŠå›¾ç‰‡è´´è¿›å»
        éœ€è¦ Pillow>=8.2ï¼ˆrounded_rectangle æ”¯æŒï¼‰
        """
        img = img.convert("RGBA")
        w, h = img.size
        mask = Image.new("L", (w, h), 0)
        md = ImageDraw.Draw(mask)
        md.rounded_rectangle([(0, 0), (w - 1, h - 1)], radius=radius, fill=255)
        white = Image.new("RGBA", (w, h), (255, 255, 255, 255))
        white.paste(img, mask=mask)
        return white.convert("RGB")

    def _draw_avatar_circle(self, im, x, y, size, char, color):
        """åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶ä¸€ä¸ªå¸¦æ–‡å­—çš„åœ†å½¢å¤´åƒ
        ç”¨4xè¶…é‡‡æ ·ç”»å¤§åœ†å†ç¼©å°ï¼Œå®ç°æŠ—é”¯é½¿çš„å¹³æ»‘åœ†å½¢è¾¹ç¼˜
        char: åœ†å¿ƒé‡Œæ˜¾ç¤ºçš„å­—ç¬¦ï¼ˆé¢‘é“åé¦–å­—ï¼‰
        color: åœ†å½¢çš„RGBèƒŒæ™¯è‰²
        """
        scale = 4
        big = Image.new("RGBA", (size * scale, size * scale), (0, 0, 0, 0))
        bd = ImageDraw.Draw(big)
        bd.ellipse([(0, 0), (size * scale - 1, size * scale - 1)], fill=color + (255,))
        big = big.resize((size, size), Image.LANCZOS)
        im.paste(big, (x, y), big)
        # åœ¨åœ†å¿ƒç”»å­—
        d = ImageDraw.Draw(im)
        font = self._f(int(size * 0.42))
        try:
            bbox = font.getbbox(char)
            cw = bbox[2] - bbox[0]
            ch = bbox[3] - bbox[1]
            d.text((x + (size - cw) / 2 - bbox[0], y + (size - ch) / 2 - bbox[1]),
                   char, font=font, fill=(255, 255, 255))
        except Exception:
            d.text((x + size // 4, y + size // 4), "?", font=font, fill=(255, 255, 255))

    def _format_time(self, ts_str):
        """æŠŠRSSçš„é•¿æ—¶é—´å­—ç¬¦ä¸²ç®€åŒ–æˆ YYYY-MM-DD HH:MM æ ¼å¼
        å¤±è´¥åˆ™åŸæ ·æˆªæ–­è¿”å›ï¼Œä¿è¯ä¸å´©æºƒ
        """
        if not ts_str:
            return ""
        try:
            from email.utils import parsedate_to_datetime
            dt = parsedate_to_datetime(ts_str)
            return dt.strftime("%Y-%m-%d %H:%M")
        except Exception:
            pass
        for fmt in ["%Y-%m-%dT%H:%M:%S%z", "%Y-%m-%dT%H:%M:%S.%f%z"]:
            try:
                dt = datetime.strptime(ts_str.replace("Z", "+0000"), fmt)
                return dt.strftime("%Y-%m-%d %H:%M")
            except Exception:
                continue
        return ts_str[:25] if len(ts_str) > 25 else ts_str
    def make(self, channel="", title="", desc="", link="", ts="", thumb=None):
        """ç”Ÿæˆ Twitter/X é£æ ¼çš„åŠ¨æ€å¡ç‰‡

        å¸ƒå±€ï¼ˆæ¨¡ä»¿æ¨ç‰¹æ—¶é—´çº¿çš„å•æ¡æ¨æ–‡ï¼‰:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  [â—]  é¢‘é“å Â· 2025-02-19 19:54  â”‚
        â”‚       æ­£æ–‡æ­£æ–‡æ­£æ–‡æ­£æ–‡æ­£æ–‡        â”‚
        â”‚       æ­£æ–‡æ­£æ–‡...                â”‚
        â”‚       â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®     â”‚
        â”‚       â”‚   å›¾ç‰‡(åœ†è§’14px)    â”‚     â”‚
        â”‚       â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯     â”‚
        â”‚  â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€  â”‚
        â”‚       ğŸ”— æ¥æºé“¾æ¥                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        å¤šæ¡æ‹¼åˆååº•éƒ¨åˆ†å‰²çº¿è¿æˆè¿ç»­æ—¶é—´çº¿ã€‚
        """
        # ============ Twitter/X ç²¾ç¡®é…è‰² ============
        BG       = (255, 255, 255)       # èƒŒæ™¯çº¯ç™½
        C_NAME   = (15, 20, 25)          # åå­—é»‘ (#0F1419)
        C_BODY   = (15, 20, 25)          # æ­£æ–‡é»‘
        C_GRAY   = (83, 100, 113)        # å‰¯æ–‡å­—ç° (#536471)
        C_BORDER = (239, 243, 244)       # åˆ†å‰²çº¿ (#EFF3F4)
        C_BLUE   = (29, 155, 240)        # Twitterè“ (#1D9BF0)

        # ============ å¸ƒå±€å¸¸é‡ ============
        W   = self.w                     # å¡ç‰‡æ€»å®½åº¦(é»˜è®¤480)
        PX  = 16                         # å·¦å³å†…è¾¹è·
        PY  = 14                         # ä¸Šä¸‹å†…è¾¹è·
        AVT = 48                         # å¤´åƒç›´å¾„
        GAP = 12                         # å¤´åƒå’Œå†…å®¹ä¹‹é—´æ°´å¹³é—´è·
        CX  = PX + AVT + GAP            # å†…å®¹åŒºèµ·å§‹Xåæ ‡
        CW  = W - CX - PX               # å†…å®¹åŒºå¯ç”¨å®½åº¦

        # ============ å­—ä½“ ============
        # fn: é¢‘é“å  ft: æ—¶é—´  fh: æ ‡é¢˜(å¤§)  fb: æ­£æ–‡  fm: åº•éƒ¨é“¾æ¥
        fn = self._f(16)                 # é¢‘é“å
        ft = self._f(12)                 # æ—¶é—´
        fh = self._f(18)                 # æ ‡é¢˜ï¼ˆæ¯”æ­£æ–‡å¤§ï¼Œè§†è§‰å±‚æ¬¡åˆ†æ˜ï¼‰
        fb = self._f(15)                 # æ­£æ–‡
        fm = self._f(12)                 # åº•éƒ¨é“¾æ¥

        # ============ 1. é¢„è®¡ç®—æ–‡æœ¬æ¢è¡Œ ============
        tmp = Image.new("RGB", (1, 1))
        d0 = ImageDraw.Draw(tmp)

        # æ ‡é¢˜å’Œæ­£æ–‡åˆ†å¼€å¤„ç†ï¼ˆè€Œä¸æ˜¯åˆå¹¶æˆä¸€æ®µï¼‰
        # è¿™æ ·æ ‡é¢˜å¯ä»¥ç”¨å¤§å­—ä½“ï¼Œæ­£æ–‡ç”¨å°å­—ä½“ï¼Œæœ‰å±‚æ¬¡æ„Ÿ
        # å¦‚æœæ ‡é¢˜å’Œæ­£æ–‡å†…å®¹é‡å¤ï¼Œåˆ™åªæ˜¾ç¤ºæ ‡é¢˜
        show_title = title and title not in ("æ— æ ‡é¢˜", "")
        title_lines = self._wrap(title, fh, CW, d0) if show_title else []
        if len(title_lines) > 4:
            title_lines = title_lines[:4]
            title_lines[-1] = title_lines[-1].rstrip() + "..."
        TITLE_LH = 30  # æ ‡é¢˜è¡Œé«˜(px)ï¼š18pxå­—ä½“ Ã— 1.67å€ â‰ˆ 30

        # æ­£æ–‡ï¼šå¦‚æœå’Œæ ‡é¢˜å®Œå…¨ç›¸åŒå°±ä¸é‡å¤æ˜¾ç¤º
        desc_text = (desc or "").strip()
        if show_title and desc_text == title.strip():
            desc_text = ""
        desc_lines = self._wrap(desc_text, fb, CW, d0) if desc_text else []
        if len(desc_lines) > 15:
            desc_lines = desc_lines[:15]
            desc_lines[-1] = desc_lines[-1].rstrip() + "..."
        DESC_LH = 26   # æ­£æ–‡è¡Œé«˜(px)ï¼š15pxå­—ä½“ Ã— 1.73å€ â‰ˆ 26ï¼Œä¸å†æŒ¤

        # ============ 2. å¤„ç†ç¼©ç•¥å›¾ ============
        pic = None
        pic_h = 0
        if thumb:
            try:
                src = Image.open(BytesIO(thumb))
                # ç»Ÿä¸€è½¬RGBAï¼Œå¤„ç†é€æ˜PNG
                if src.mode != "RGBA":
                    src = src.convert("RGBA")

                ratio = CW / src.width
                new_h = int(src.height * ratio)
                # é™åˆ¶æœ€å¤§é«˜åº¦ï¼Œé˜²ç«–é•¿å›¾æ’‘çˆ†å¡ç‰‡
                max_h = int(CW * 1.3)
                src = src.resize((CW, min(new_h, max_h)), Image.LANCZOS)
                if new_h > max_h:
                    src = src.crop((0, 0, CW, max_h))
                    new_h = max_h

                # æŠŠé€æ˜å›¾åˆæˆåˆ°ç™½åº•ä¸Šï¼ˆé˜²æ­¢é€æ˜åŒºåŸŸå˜é»‘ï¼‰
                white_bg = Image.new("RGBA", (CW, min(new_h, max_h)), (255, 255, 255, 255))
                try:
                    white_bg.paste(src, mask=src.split()[3])
                except Exception:
                    white_bg.paste(src)
                # åŠ åœ†è§’
                pic = self._round_image(white_bg.convert("RGB"), radius=14)
                pic_h = pic.height
            except Exception:
                pic = None

        # æ ¼å¼åŒ–æ—¶é—´
        time_str = self._format_time(ts)

        # ============ 3. è®¡ç®—æ€»é«˜åº¦ ============
        # é€å—ç´¯åŠ ï¼šä¸Šè¾¹è· â†’ å¤´åƒåŒº â†’ æ ‡é¢˜ â†’ æ­£æ–‡ â†’ å›¾ç‰‡ â†’ åˆ†å‰²çº¿ â†’ é“¾æ¥ â†’ ä¸‹è¾¹è·
        H = PY                                                 # ä¸Šè¾¹è·
        H += max(AVT, 24) + 10                                 # å¤´åƒ/åå­—åŒº + é—´è·
        if title_lines:
            H += len(title_lines) * TITLE_LH + 8              # æ ‡é¢˜å— + åº•éƒ¨é—´è·
        if desc_lines:
            H += len(desc_lines) * DESC_LH + 10               # æ­£æ–‡å— + åº•éƒ¨é—´è·
        if pic:
            H += pic_h + 14                                    # å›¾ç‰‡ + åº•éƒ¨é—´è·
        H += 1 + 10                                            # åˆ†å‰²çº¿ + é—´è·
        if link:
            H += 18 + 4                                        # é“¾æ¥è¡Œ
        H += PY                                                # ä¸‹è¾¹è·                          # ä¸‹è¾¹è·

        # ============ 4. ç»˜åˆ¶ç”»å¸ƒ ============
        im = Image.new("RGB", (W, H), BG)
        dr = ImageDraw.Draw(im)
        cy = PY  # å½“å‰Yæ¸¸æ ‡

        # ---- å¤´åƒ ----
        avt_char = "?"
        for c in (channel or ""):
            if c.strip():
                avt_char = c
                break
        self._draw_avatar_circle(im, PX, cy, AVT, avt_char, C_BLUE)

        # ---- é¢‘é“å + æ—¶é—´ï¼ˆåŒä¸€è¡Œï¼Œæ¨¡ä»¿æ¨ç‰¹ "Name Â· 2h"ï¼‰ ----
        # [ä¿®å¤] å¼ºåˆ¶æˆªæ–­è¶…é•¿é¢‘é“åï¼Œé˜²æ­¢å’Œæ—¶é—´é‡å ä¹±ç 
        name_y = cy + (AVT - 20) // 2  # å‚ç›´å±…ä¸­äºå¤´åƒ
        
        display_name = channel or "æœªçŸ¥é¢‘é“"
        # å»æ‰RSSHubå¯èƒ½é™„åŠ çš„å†—ä½™åç¼€ï¼Œè®©åå­—æ›´çŸ­æ›´å¹²å‡€
        display_name = display_name.replace(" - Community Posts - YouTube", "").replace(" - YouTube", "")
        
        if time_str:
            dot = " Â· "
            # é¢„ç•™ç»™æ—¶é—´å’Œç‚¹çš„å®½åº¦
            time_w = d0.textlength(time_str, font=ft)
            dot_w = d0.textlength(dot, font=ft)
            
            # è®¡ç®—åå­—æœ€å¤§å…è®¸å®½åº¦ = æ€»å®½åº¦ - æ—¶é—´å®½ - ç‚¹å®½ - ç¼“å†²(10px)
            max_name_w = CW - time_w - dot_w - 10
            
            # æµ‹é‡å½“å‰åå­—å®½åº¦
            current_w = d0.textlength(display_name, font=fn)
            
            # å¦‚æœåå­—å¤ªé•¿ï¼Œå°±å¾ªç¯æˆªæ–­ç›´åˆ°æ”¾å¾—ä¸‹
            if current_w > max_name_w:
                while current_w > max_name_w and len(display_name) > 1:
                    display_name = display_name[:-1]
                    current_w = d0.textlength(display_name + "...", font=fn)
                display_name += "..."
            
            # ç»˜åˆ¶åå­—
            dr.text((CX, name_y), display_name, font=fn, fill=C_NAME)
            
            # ç´§æ¥ç€ç»˜åˆ¶ Â· æ—¶é—´
            final_name_w = d0.textlength(display_name, font=fn)
            dr.text((CX + final_name_w, name_y + 1), dot, font=ft, fill=C_GRAY)
            dr.text((CX + final_name_w + dot_w, name_y + 1), time_str, font=ft, fill=C_GRAY)
        else:
            # æ²¡æœ‰æ—¶é—´ï¼Œç›´æ¥ç”»åå­—ï¼ˆä¹Ÿè¦é˜²æ­¢è¶…é•¿ï¼‰
            current_w = d0.textlength(display_name, font=fn)
            if current_w > CW:
                while current_w > CW and len(display_name) > 1:
                    display_name = display_name[:-1]
                    current_w = d0.textlength(display_name + "...", font=fn)
                display_name += "..."
            dr.text((CX, name_y), display_name, font=fn, fill=C_NAME)

        cy += max(AVT, 24) + 10

        # ---- æ ‡é¢˜ï¼ˆå¤§å­—ï¼Œæ·±é»‘ï¼‰ ----
        if title_lines:
            for line in title_lines:
                dr.text((CX, cy), line, font=fh, fill=C_NAME)
                cy += TITLE_LH
            cy += 8

        # ---- æ­£æ–‡ï¼ˆå°å­—ï¼Œæ·±ç°ï¼Œå’Œæ ‡é¢˜å½¢æˆå¯¹æ¯”ï¼‰ ----
        if desc_lines:
            for line in desc_lines:
                dr.text((CX, cy), line, font=fb, fill=C_GRAY)
                cy += DESC_LH
            cy += 10

        # ---- å›¾ç‰‡ï¼ˆåœ†è§’ï¼‰ ----
        if pic:
            im.paste(pic, (CX, cy))
            # åŠ åœ†è§’è¾¹æ¡†çº¿ï¼Œè®©å›¾ç‰‡è¾¹ç¼˜æ›´æ¸…æ™°
            dr.rounded_rectangle(
                [(CX, cy), (CX + CW - 1, cy + pic_h - 1)],
                radius=14, outline=C_BORDER, width=1
            )
            cy += pic_h + 14

        # ---- åˆ†å‰²çº¿ ----
        dr.line([(PX, cy), (W - PX, cy)], fill=C_BORDER, width=1)
        cy += 10

        # ---- é“¾æ¥ ----
        if link:
            lk = link if len(link) <= 50 else link[:50] + "..."
            dr.text((CX, cy), "ğŸ”— " + lk, font=fm, fill=C_BLUE)
            cy += 22

        # åº•éƒ¨è¾¹çº¿ï¼ˆå¤šæ¡æ‹¼åˆæ—¶å……å½“æ¡ç›®é—´åˆ†éš”çº¿ï¼Œåƒæ¨ç‰¹æ—¶é—´çº¿çš„ç°çº¿ï¼‰
        dr.line([(0, H - 1), (W, H - 1)], fill=C_BORDER, width=1)

        buf = BytesIO()
        im.save(buf, format="PNG")
        buf.seek(0)
        return base64.b64encode(buf.read()).decode()

@register("astrbot_plugin_myrss", "MyRSS", "RSSè®¢é˜…æ’ä»¶(LLMå¢å¼ºç‰ˆ)", "1.0.0", "")
class MyRssPlugin(Star):
    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.logger = logging.getLogger("astrbot")
        self.ctx = context
        self.cfg = config
        self.dh = DataHandler()

        self.title_max = config.get("title_max_length", 60)
        self.desc_max = config.get("description_max_length", 200)
        self.max_poll = config.get("max_items_per_poll", 5)
        self.t2i = config.get("t2i", False)
        self.hide_url = config.get("is_hide_url", False)
        self.read_pic = config.get("is_read_pic", True)
        self.adjust_pic = config.get("is_adjust_pic", False)
        self.max_pic = config.get("max_pic_item", 3)
        self.compose = config.get("compose", True)

        self.pic = PicHandler(self.adjust_pic)
        self.card = CardGen()

        # é˜²å¹¶å‘é”ï¼Œkey = (url, user)
        self._locks: dict = {}

        self.sched = AsyncIOScheduler()
        self.sched.start()
        self._reload_jobs()

    async def destroy(self):
        """æ’ä»¶å¸è½½/ç¦ç”¨æ—¶åœæ­¢è°ƒåº¦å™¨"""
        try:
            if self.sched.running:
                self.sched.shutdown(wait=False)
                self.logger.info("MyRSS: è°ƒåº¦å™¨å·²åœæ­¢")
        except Exception as e:
            self.logger.error("MyRSS: åœæ­¢è°ƒåº¦å™¨å¤±è´¥: %s", e)

    def _get_lock(self, url: str, user: str) -> asyncio.Lock:
        key = (url, user)
        if key not in self._locks:
            self._locks[key] = asyncio.Lock()
        return self._locks[key]

    def _cron(self, expr: str) -> dict:
        f = expr.split(" ")
        return {"minute": f[0], "hour": f[1], "day": f[2], "month": f[3], "day_of_week": f[4]}

    def _reload_jobs(self) -> None:
        self.sched.remove_all_jobs()
        for url, info in self.dh.data.items():
            if url in ("rsshub_endpoints", "settings"):
                continue
            subs = info.get("subscribers", {})
            if not subs:
                continue
            # å–æ‰€æœ‰è®¢é˜…è€…ä¸­é—´éš”æœ€å¤§çš„cronï¼ˆæœ€ä¿å®ˆï¼Œå‡å°‘æ‹‰å–é¢‘ç‡ï¼‰
            def cron_to_hours(expr: str) -> int:
                """ä» '0 */N * * *' æå–Nï¼Œå¤±è´¥è¿”å›1"""
                try:
                    return int(expr.split(" ")[1].replace("*/", ""))
                except Exception:
                    return 1

            max_hours = max(cron_to_hours(si["cron_expr"]) for si in subs.values())
            merged_cron = f"0 */{max_hours} * * *"
            # æ¯ä¸ªURLåªæ³¨å†Œä¸€ä¸ªjobï¼Œæ‹‰å–ååˆ†å‘ç»™æ‰€æœ‰è®¢é˜…è€…
            self.sched.add_job(
                self._cron_cb_url, "cron",
                **self._cron(merged_cron),
                args=[url]
            )
            self.logger.info("RSSè°ƒåº¦: %s æ¯%då°æ—¶æ‹‰å–ï¼Œ%dä¸ªè®¢é˜…è€…", url, max_hours, len(subs))

    async def _fetch(self, url: str):
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        conn = aiohttp.TCPConnector(ssl=False)
        to = aiohttp.ClientTimeout(total=30, connect=10)

        async def _try(u: str):
            try:
                async with aiohttp.ClientSession(trust_env=True, connector=conn, timeout=to, headers=headers) as s:
                    async with s.get(u) as r:
                        if r.status != 200:
                            return None
                        return await r.read()
            except Exception:
                return None

        data = await _try(url)
        if data is not None:
            return data

        eps = self.dh.data.get("rsshub_endpoints", [])
        if not eps:
            return None

        parsed = urlparse(url)
        path = parsed.path + (("?" + parsed.query) if parsed.query else "")
        cur_root = f"{parsed.scheme}://{parsed.netloc}"
        norm_eps = [(e[:-1] if e.endswith("/") else e) for e in eps]

        for ep in norm_eps:
            if ep == cur_root:
                continue
            alt = ep + path
            data = await _try(alt)
            if data is not None:
                self.logger.warning("rss: ç«¯ç‚¹ä¸å¯ç”¨ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢ %s -> %s", url, alt)
                return data

        return None

    def _parse_pubdate(self, pd: str):
        """è§£æå„ç§æ—¥æœŸæ ¼å¼ï¼Œå¤±è´¥è¿”å›None"""
        if not pd:
            return None
        pd = pd.strip()

        # ä¼˜å…ˆç”¨æ ‡å‡†åº“çš„RFC2822è§£æå™¨ï¼ˆæœ€ç¨³ï¼Œä¸å—localeå½±å“ï¼‰
        try:
            from email.utils import parsedate_to_datetime
            dt = parsedate_to_datetime(pd)
            return int(dt.timestamp())
        except Exception:
            pass

        # è¡¥å……ISO8601ç­‰æ ¼å¼
        pd_clean = pd.replace("GMT", "+0000").replace("Z", "+0000")
        for fmt in [
            "%a, %d %b %Y %H:%M:%S %z",
            "%Y-%m-%dT%H:%M:%S%z",
            "%Y-%m-%dT%H:%M:%S.%f%z",
            "%Y-%m-%d %H:%M:%S%z",
            "%Y-%m-%d %H:%M:%S",
        ]:
            try:
                dt = datetime.strptime(pd_clean, fmt)
                return int(dt.timestamp())
            except Exception:
                continue
        return None

    async def _poll(self, url: str, num: int = -1, after_ts: int = 0, after_link: str = "") -> List[RSSItem]:
        text = await self._fetch(url)
        if text is None:
            return []
        try:
            root = etree.fromstring(text)
        except ValueError:
            try:
                root = etree.fromstring(
                    text.replace(b'encoding="gb2312"', b'')
                        .replace(b'encoding="GB2312"', b'')
                )
            except Exception:
                return []

        items = root.xpath("//item")
        ns = {"media": "http://search.yahoo.com/mrss/"}
        result = []
        cnt = 0

        for it in items:
            try:
                ch = self.dh.data[url]["info"]["title"] if url in self.dh.data else "æœªçŸ¥"

                tn = it.xpath("title")
                title = tn[0].text if tn else "æ— æ ‡é¢˜"
                if len(title) > self.title_max:
                    title = title[:self.title_max] + "..."

                ln = it.xpath("link")
                link = (ln[0].text or "").strip() if ln else ""
                if link and not re.match(r"^https?://", link):
                    link = self.dh.get_root_url(url) + link

                dn = it.xpath("description")
                raw = dn[0].text if dn else ""
                pics = self.dh.strip_html_pic(raw) if raw else []
                desc = self.dh.strip_html(raw) if raw else ""
                if len(desc) > self.desc_max:
                    desc = desc[:self.desc_max] + "..."

                # [å¢å¼º] ä»å¤šç§XMLæ ‡ç­¾æå–å›¾ç‰‡URL
                # media:thumbnail  â†’ RSSæ ‡å‡†ç¼©ç•¥å›¾ï¼ˆè§†é¢‘è·¯ç”±å¸¸ç”¨ï¼‰
                # media:content    â†’ æœ‰äº›æºæŠŠå°é¢å›¾æ”¾è¿™é‡Œï¼ˆYouTubeç­‰ï¼‰
                # enclosure        â†’ RSSé™„ä»¶
                # local-name()é€šé… â†’ å…¼å®¹ä¸åŒå‘½åç©ºé—´å†™æ³•
                for u in (
                    it.xpath("media:thumbnail/@url", namespaces=ns)
                    + it.xpath("media:content/@url", namespaces=ns)
                    + it.xpath("media:content/media:thumbnail/@url", namespaces=ns)
                    + it.xpath(".//*[local-name()='thumbnail']/@url")
                    + it.xpath(".//*[local-name()='content']/@url")
                    + it.xpath("enclosure[contains(@type,'image')]/@url")
                    + it.xpath("enclosure/@url")
                ):
                    if u and u not in pics:
                        # è¿‡æ»¤æ‰è§†é¢‘/éŸ³é¢‘æ–‡ä»¶ï¼Œåªä¿ç•™å›¾ç‰‡
                        low = u.lower()
                        if not any(low.endswith(e) for e in ('.mp4', '.webm', '.mp3', '.m4a', '.ogg')):
                            pics.append(u)

                pub_nodes = it.xpath("pubDate")
                if pub_nodes:
                    pd = pub_nodes[0].text or ""
                    pts = self._parse_pubdate(pd)

                    if pts is None:
                        # è§£æå¤±è´¥ï¼Œç”¨ link å…œåº•å»é‡
                        if link and link != after_link:
                            result.append(RSSItem(ch, title, link, desc, pd, 0, pics))
                            cnt += 1
                    elif pts > after_ts:
                        result.append(RSSItem(ch, title, link, desc, pd, pts, pics))
                        cnt += 1
                    else:
                        break
                else:
                    if link and link != after_link:
                        result.append(RSSItem(ch, title, link, desc, "", 0, pics))
                        cnt += 1
                    else:
                        break

                if num != -1 and cnt >= num:
                    break

            except Exception as e:
                self.logger.error("rss: è§£ææ¡ç›®å¤±è´¥ %s: %s", url, e)
                break

        return result

    async def _add(self, url: str, cron_expr: str, event: AstrMessageEvent):
        user = event.unified_msg_origin
        if url in self.dh.data:
            items = await self._poll(url)
            if not items:
                return event.plain_result("æ— æ³•ä»è¯¥æºè·å–å†…å®¹ï¼Œè¯·æ£€æŸ¥é“¾æ¥ã€‚")
            self.dh.data[url]["subscribers"][user] = {
                "cron_expr": cron_expr,
                "last_update": items[0].pubDate_timestamp,
                "latest_link": items[0].link,
                "seen_links": [it.link for it in items if it.link][:200],
            }
        else:
            text = await self._fetch(url)
            if text is None:
                return event.plain_result("æ— æ³•è®¿é—®: " + url + "\nè¯·æ£€æŸ¥RSSHubç«¯ç‚¹æ˜¯å¦å¯ç”¨ã€‚")
            try:
                title, desc = self.dh.parse_channel_info(text)
            except Exception as e:
                return event.plain_result("è§£æå¤±è´¥: " + str(e))
            items = await self._poll(url)
            if not items:
                return event.plain_result("æºå¯è®¿é—®ä½†æ— å†…å®¹æ¡ç›®ã€‚")
            self.dh.data[url] = {
                "subscribers": {
                    user: {
                        "cron_expr": cron_expr,
                        "last_update": items[0].pubDate_timestamp,
                        "latest_link": items[0].link,
                    "seen_links": [it.link for it in items if it.link][:200],
                }
            },
            "info": {"title": title, "description": desc},
            }
        self.dh.save()
        return self.dh.data[url]["info"]

    async def _make_card_b64(self, item: RSSItem) -> str:
        tb = None
        if self.read_pic and item.pic_urls:
            # [ä¿®æ”¹] éå†å›¾ç‰‡åˆ—è¡¨å°è¯•ä¸‹è½½ï¼Œç›´åˆ°æˆåŠŸä¸€ä¸ª
            # è§£å†³YouTubeå°é¢å¯èƒ½æ˜¯404çš„é—®é¢˜
            conn = aiohttp.TCPConnector(ssl=False)
            async with aiohttp.ClientSession(trust_env=True, connector=conn) as s:
                for pu in item.pic_urls:
                    try:
                        async with s.get(pu, timeout=aiohttp.ClientTimeout(total=5)) as r:
                            if r.status == 200:
                                data = await r.read()
                                # ç®€å•æ ¡éªŒæ•°æ®é•¿åº¦ï¼Œé˜²æ­¢ä¸‹è½½åˆ°ç©ºçš„
                                if len(data) > 100: 
                                    tb = data
                                    break
                    except Exception:
                        continue
        return self.card.make(
            channel=item.chan_title,
            title=item.title,
            desc=item.description,
            link="" if self.hide_url else item.link,
            ts=item.pubDate or "",
            thumb=tb,
        )

    def _merge_cards_b64(self, cards_b64: list) -> str:
        imgs = []
        for b64 in cards_b64:
            raw = base64.b64decode(b64)
            imgs.append(Image.open(BytesIO(raw)).convert("RGB"))

        if not imgs:
            return ""

        width = max(im.width for im in imgs)
        # [ä¿®æ”¹] é—´è·è®¾ä¸º0ï¼Œè®©æ¯æ¡å¡ç‰‡åº•éƒ¨è‡ªå¸¦çš„åˆ†å‰²çº¿ç›´æ¥å……å½“
        # æ¡ç›®ä¹‹é—´çš„åˆ†éš”ï¼Œæ‹¼å‡ºæ¥å°±åƒæ¨ç‰¹æ—¶é—´çº¿ä¸€æ ·æ— ç¼è¡”æ¥
        pad = 0
        resized = []
        total_h = 0
        for im in imgs:
            if im.width != width:
                nh = int(im.height * (width / im.width))
                im = im.resize((width, nh), Image.LANCZOS)
            resized.append(im)
            total_h += im.height

        # [ä¿®æ”¹] ç™½åº•ç”»å¸ƒï¼Œé—´è·ä¸º0ç´§å¯†æ‹¼æ¥
        canvas = Image.new("RGB", (width, total_h), (255, 255, 255))
        y = 0
        for im in resized:
            canvas.paste(im, (0, y))
            y += im.height

        out = BytesIO()
        canvas.save(out, format="PNG")
        out.seek(0)
        return base64.b64encode(out.read()).decode("utf-8")

    async def _make_comps(self, item: RSSItem) -> list:
        comps = []
        tb = None
        if self.read_pic and item.pic_urls:
            try:
                conn = aiohttp.TCPConnector(ssl=False)
                async with aiohttp.ClientSession(trust_env=True, connector=conn) as s:
                    async with s.get(item.pic_urls[0], timeout=aiohttp.ClientTimeout(total=15)) as r:
                        if r.status == 200:
                            tb = await r.read()
            except Exception:
                pass
        try:
            b64 = self.card.make(
                channel=item.chan_title, title=item.title, desc=item.description,
                link="" if self.hide_url else item.link, ts=item.pubDate or "", thumb=tb,
            )
            comps.append(Comp.Image.fromBase64(b64))
        except Exception as e:
            self.logger.error("å¡ç‰‡ç”Ÿæˆå¤±è´¥: %s", e)
            comps.append(Comp.Plain("ğŸ“¡ " + item.chan_title + "\nğŸ“ " + item.title + "\n" + item.description))

        if self.read_pic and item.pic_urls:
            mx = len(item.pic_urls) if self.max_pic == -1 else self.max_pic
            for pu in item.pic_urls[1:mx]:
                try:
                    b = await self.pic.to_base64(pu)
                    if b:
                        comps.append(Comp.Image.fromBase64(b))
                except Exception:
                    pass
        return comps

    async def _cron_cb_url(self, url: str) -> None:
        """æ¯ä¸ªURLåªæ‹‰å–ä¸€æ¬¡ï¼Œç»“æœåˆ†å‘ç»™æ‰€æœ‰è®¢é˜…è€…"""
        if url not in self.dh.data:
            return
        subs = self.dh.data[url].get("subscribers", {})
        if not subs:
            return

        self.logger.info("RSSå…¬å…±æ‹‰å–: %s -> %dä¸ªè®¢é˜…è€…", url, len(subs))

        # æ‰€æœ‰è®¢é˜…è€…ä¸­æœ€æ—©çš„ last_updateï¼ˆæ‹‰æœ€å¤šå†…å®¹ï¼Œå†å„è‡ªè¿‡æ»¤ï¼‰
        min_ts = min(si.get("last_update", 0) for si in subs.values())
        min_link = ""  # å…¬å…±æ‹‰å–ä¸ç”¨after_linkè¿‡æ»¤ï¼Œé seen_linkså»é‡

        items = await self._poll(url, num=self.max_poll, after_ts=min_ts, after_link=min_link)
        if not items:
            return

        # åˆ†å‘ç»™æ¯ä¸ªè®¢é˜…è€…ï¼ˆå„è‡ªç‹¬ç«‹å»é‡ï¼‰
        for user in list(subs.keys()):
            lock = self._get_lock(url, user)
            async with lock:
                await self._cron_cb_inner(url, user, prefetched_items=items)

    async def _cron_cb(self, url: str, user: str) -> None:
        """å¸¦é”çš„å®šæ—¶å›è°ƒå…¥å£ï¼Œé˜²æ­¢åŒä¸€è®¢é˜…å¹¶å‘æ‰§è¡Œ"""
        lock = self._get_lock(url, user)
        async with lock:
            await self._cron_cb_inner(url, user)

    async def _cron_cb_inner(self, url: str, user: str, prefetched_items=None) -> None:
        if url not in self.dh.data or user not in self.dh.data[url].get("subscribers", {}):
            return

        self.logger.info("RSSå®šæ—¶è§¦å‘: %s -> %s", url, user)
        si = self.dh.data[url]["subscribers"][user]

        if prefetched_items is not None:
            # ä½¿ç”¨å…¬å…±æ‹‰å–çš„ç»“æœï¼Œå†æŒ‰è¯¥ç”¨æˆ·çš„æ–­ç‚¹è¿‡æ»¤ä¸€æ¬¡
            items = [
                it for it in prefetched_items
                if it.pubDate_timestamp > si.get("last_update", 0)
                or (it.pubDate_timestamp == 0 and it.link != si.get("latest_link", ""))
            ]
        else:
            items = await self._poll(
                url,
                num=self.max_poll,
                after_ts=si["last_update"],
                after_link=si["latest_link"],
            )
        if not items:
            return

        def item_key(it: RSSItem) -> str:
            return it.link if it.link else f"{it.title}|{it.pubDate_timestamp}"

        # å»é‡
        seen = set(si.get("seen_links", []))
        new_items = [it for it in items if item_key(it) not in seen]

        if not new_items:
            si["latest_link"] = items[0].link
            self.dh.save()
            return

        # å…ˆæ›´æ–°å»é‡è®°å½•å†å‘é€ï¼Œé˜²æ­¢å¹¶å‘é‡æ¨
        new_keys = [item_key(it) for it in new_items]
        si["seen_links"] = (new_keys + si.get("seen_links", []))[:200]
        si["latest_link"] = items[0].link
        ts_candidates = [it.pubDate_timestamp for it in new_items if it.pubDate_timestamp > 0]
        if ts_candidates:
            si["last_update"] = max(ts_candidates)
        self.dh.save()

        pn = user.split(":")[0]
        merge_limit = 5
        batch = new_items[:merge_limit]

        if len(batch) > 1:
            cards = [await self._make_card_b64(it) for it in batch]
            merged = self._merge_cards_b64(cards)
            if not merged:
                for it in batch:
                    comps = await self._make_comps(it)
                    await self.ctx.send_message(user, MessageChain(chain=comps, use_t2i_=self.t2i))
            else:
                comps = [Comp.Image.fromBase64(merged)]
                if pn == "aiocqhttp" and self.compose:
                    node = Comp.Node(uin=0, name="Astrbot", content=comps)
                    await self.ctx.send_message(user, MessageChain(chain=[node], use_t2i_=self.t2i))
                else:
                    await self.ctx.send_message(user, MessageChain(chain=comps, use_t2i_=self.t2i))
        else:
            it = batch[0]
            comps = await self._make_comps(it)
            if pn == "aiocqhttp" and self.compose:
                node = Comp.Node(uin=0, name="Astrbot", content=comps)
                await self.ctx.send_message(user, MessageChain(chain=[node], use_t2i_=self.t2i))
            else:
                await self.ctx.send_message(user, MessageChain(chain=comps, use_t2i_=self.t2i))

        self.logger.info("RSSæ¨é€å®Œæˆ: %s -> %s (%dæ¡)", url, user, len(batch))

    # ============================================================
    #  LLM å·¥å…·
    # ============================================================

    @filter.llm_tool(name="myrss_subscribe")
    async def tool_sub(self, event: AstrMessageEvent, url: str = "https://example.com", interval: int = 1):
        """ç”¨æˆ·æƒ³è®¢é˜…ã€å…³æ³¨ã€è¿½è¸ªæŸä¸ªç½‘ç«™æˆ–åšä¸»æ›´æ–°æ—¶è°ƒç”¨ã€‚ä¼ å…¥ç”¨æˆ·ç»™çš„é“¾æ¥å³å¯ã€‚
    
        Args:
            url(string): ç”¨æˆ·æä¾›çš„é“¾æ¥æˆ–è·¯å¾„
            interval(int): æ£€æŸ¥é—´éš”(å°æ—¶)ï¼Œé»˜è®¤1
        """
        if not url or url == "https://example.com":
            yield event.plain_result(
                "è¯·è®©ç”¨æˆ·æä¾›å…·ä½“é“¾æ¥ã€‚æ”¯æŒä»¥ä¸‹å¹³å°è‡ªåŠ¨è¯†åˆ«ï¼š\n"
                "Bç«™(space.bilibili.com/UID)ã€YouTubeã€Twitter/Xã€å¾®åšã€çŸ¥ä¹ã€"
                "å°çº¢ä¹¦ã€GitHubã€Telegramã€æŠ–éŸ³ã€Instagramã€Pixivç­‰ã€‚\n"
                "ä¹Ÿå¯ä½¿ç”¨ /å¼€å¤´çš„RSSHubè·¯ç”±è·¯å¾„ï¼Œå¦‚ /bilibili/weekly\n"
                "è¯¦è§ https://docs.rsshub.app"
            )
            return
        eps = self.dh.data.get("rsshub_endpoints", [])
        if not eps:
            yield event.plain_result(
                "å°šæœªé…ç½®RSSHubç«¯ç‚¹ï¼Œè¯·å‘Šè¯‰ç”¨æˆ·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ä¹‹ä¸€ï¼š\n"
                "/myrss rsshub add https://rsshub.rssforever.com\n"
                "/myrss rsshub add https://rsshub.app\n"
                "é…ç½®åå³å¯è®¢é˜…ã€‚"
            )
            return
        if url.startswith("/"):
            furl = eps[0] + url
        elif url.startswith("http"):
            r = URLMapper.match(url)
            if r:
                route, pn = r
                furl = eps[0] + route
            else:
                yield event.plain_result("æ— æ³•è‡ªåŠ¨è¯†åˆ«è¯¥é“¾æ¥ã€‚\n\n" + URLMapper.suggest(url) + "\n\nè¯·é€‰æ‹©è·¯ç”±åç”¨/å¼€å¤´å†æ¬¡è°ƒç”¨ã€‚")
                return
        else:
            yield event.plain_result("è¯·æä¾›httpå¼€å¤´çš„é“¾æ¥æˆ–/å¼€å¤´çš„è·¯ç”±ã€‚")
            return
        if interval < 1:
            interval = 1
        # å¦‚æœå·²æœ‰è®¢é˜…è€…ï¼Œé—´éš”åªèƒ½å–æ›´å¤§å€¼ï¼ˆä¿æŠ¤å…¬å…±æºï¼‰
        if furl in self.dh.data:
            existing_subs = self.dh.data[furl].get("subscribers", {})
            if existing_subs:
                def cron_to_hours(expr: str) -> int:
                    try:
                        return int(expr.split(" ")[1].replace("*/", ""))
                    except Exception:
                        return 1
                max_existing = max(cron_to_hours(si["cron_expr"]) for si in existing_subs.values())
                if interval < max_existing:
                    interval = max_existing
                    yield event.plain_result(f"âš ï¸ å·²æœ‰è®¢é˜…è€…ä½¿ç”¨{max_existing}å°æ—¶é—´éš”ï¼Œä¸ºä¿æŠ¤å…¬å…±æºå·²è‡ªåŠ¨è°ƒæ•´ä¸º{max_existing}å°æ—¶ã€‚")
        ret = await self._add(furl, "0 */" + str(interval) + " * * *", event)
        if isinstance(ret, MessageEventResult):
            yield ret
            return
        self._reload_jobs()
        yield event.plain_result("âœ… è®¢é˜…æˆåŠŸï¼\nğŸ“¡ " + ret["title"] + "\nğŸ“ " + ret["description"] + "\nâ° æ¯" + str(interval) + "å°æ—¶\nğŸ”— " + furl)

    @filter.llm_tool(name="myrss_list")
    async def tool_list(self, event: AstrMessageEvent, query: str = "all"):
        """ç”¨æˆ·é—®è®¢é˜…äº†ä»€ä¹ˆæ—¶è°ƒç”¨ã€‚
    
        Args:
            query(string): å›ºå®šä¼ all
        """
        user = event.unified_msg_origin
        urls = self.dh.get_subs(user)
        if not urls:
            yield event.plain_result("å½“å‰æ²¡æœ‰ä»»ä½•è®¢é˜…ã€‚")
            return
        txt = "ğŸ“‹ è®¢é˜…åˆ—è¡¨ï¼š\n"
        for i, u in enumerate(urls):
            info = self.dh.data[u]["info"]
            cr = self.dh.data[u]["subscribers"][user]["cron_expr"]
            txt += "  " + str(i) + ". " + info["title"] + " [" + cr + "]\n"
        yield event.plain_result(txt)

    @filter.llm_tool(name="myrss_unsubscribe")
    async def tool_unsub(self, event: AstrMessageEvent, idx: int = 0):
        """å–æ¶ˆè®¢é˜…ï¼Œå…ˆè°ƒç”¨myrss_listè·å–ç¼–å·ã€‚
    
        Args:
            idx(int): è®¢é˜…ç¼–å·
        """
        user = event.unified_msg_origin
        urls = self.dh.get_subs(user)
        if idx < 0 or idx >= len(urls):
            yield event.plain_result("ç¼–å·" + str(idx) + "ä¸å­˜åœ¨ï¼Œæœ‰æ•ˆèŒƒå›´0~" + str(len(urls) - 1))
            return
        u = urls[idx]
        t = self.dh.data[u]["info"]["title"]
        self.dh.data[u]["subscribers"].pop(user)
        self.dh.save()
        self._reload_jobs()
        yield event.plain_result("âœ… å·²å–æ¶ˆ: " + t)

    # ============================================================
    #  æ‰‹åŠ¨å‘½ä»¤
    # ============================================================

    @filter.command_group("myrss")
    def myrss(self):
        pass

    @myrss.group("rsshub")
    def rsshub(self, event: AstrMessageEvent):
        pass

    @rsshub.command("add")
    async def rsshub_add(self, event: AstrMessageEvent, url: str):
        """æ·»åŠ RSSHubç«¯ç‚¹"""
        if url.endswith("/"):
            url = url[:-1]
        if url in self.dh.data["rsshub_endpoints"]:
            yield event.plain_result("å·²å­˜åœ¨")
            return
        self.dh.data["rsshub_endpoints"].append(url)
        self.dh.save()
        yield event.plain_result("âœ… å·²æ·»åŠ : " + url)

    @rsshub.command("list")
    async def rsshub_list(self, event: AstrMessageEvent):
        """åˆ—å‡ºæ‰€æœ‰RSSHubç«¯ç‚¹"""
        eps = self.dh.data["rsshub_endpoints"]
        if not eps:
            yield event.plain_result("æš‚æ— ç«¯ç‚¹ï¼Œè¯·å…ˆ /myrss rsshub add <url>")
            return
        txt = "RSSHubç«¯ç‚¹ï¼š\n"
        for i, x in enumerate(eps):
            txt += "  " + str(i) + ": " + x + "\n"
        yield event.plain_result(txt)

    @rsshub.command("remove")
    async def rsshub_rm(self, event: AstrMessageEvent, idx: int):
        """åˆ é™¤RSSHubç«¯ç‚¹"""
        eps = self.dh.data["rsshub_endpoints"]
        if idx < 0 or idx >= len(eps):
            yield event.plain_result("ç¼–å·è¶Šç•Œ")
            return
        removed = eps.pop(idx)
        self.dh.save()
        yield event.plain_result("âœ… å·²åˆ é™¤: " + removed)

    @myrss.command("list")
    async def cmd_list(self, event: AstrMessageEvent):
        """åˆ—å‡ºå½“å‰è®¢é˜…"""
        user = event.unified_msg_origin
        urls = self.dh.get_subs(user)
        if not urls:
            yield event.plain_result("æš‚æ— è®¢é˜…")
            return
        txt = "è®¢é˜…åˆ—è¡¨ï¼š\n"
        for i, u in enumerate(urls):
            info = self.dh.data[u]["info"]
            txt += "  " + str(i) + ". " + info["title"] + "\n"
        yield event.plain_result(txt)

    @myrss.command("remove")
    async def cmd_rm(self, event: AstrMessageEvent, idx: int):
        """å–æ¶ˆè®¢é˜…"""
        user = event.unified_msg_origin
        urls = self.dh.get_subs(user)
        if idx < 0 or idx >= len(urls):
            yield event.plain_result("ç¼–å·è¶Šç•Œ")
            return
        u = urls[idx]
        t = self.dh.data[u]["info"]["title"]
        self.dh.data[u]["subscribers"].pop(user)
        self.dh.save()
        self._reload_jobs()
        yield event.plain_result("âœ… å·²å–æ¶ˆ: " + t)

    @myrss.command("get")
    async def cmd_get(self, event: AstrMessageEvent, idx: int):
        """è·å–æœ€æ–°å†…å®¹"""
        user = event.unified_msg_origin
        urls = self.dh.get_subs(user)
        if idx < 0 or idx >= len(urls):
            yield event.plain_result("ç¼–å·è¶Šç•Œ")
            return
        items = await self._poll(urls[idx])
        if not items:
            yield event.plain_result("æš‚æ— å†…å®¹")
            return
        comps = await self._make_comps(items[0])
        pn = user.split(":")[0]
        if pn == "aiocqhttp" and self.compose:
            yield event.chain_result([Comp.Node(uin=0, name="Astrbot", content=comps)]).use_t2i(self.t2i)
        else:
            yield event.chain_result(comps).use_t2i(self.t2i)
